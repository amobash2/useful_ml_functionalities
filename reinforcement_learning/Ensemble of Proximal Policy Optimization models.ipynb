{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb758b7",
   "metadata": {},
   "source": [
    "### Ensemble PPO models\n",
    "In this notebook, we share a simple example on how we can ensemble multiple Proximal Policy Optimization models at inference time to improve performance of our Reinforcement Learning agent.\n",
    "\n",
    "For illustration, we have selected a simple Lundar Lander example with discrete action space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493255a",
   "metadata": {},
   "source": [
    "First, let's install required packages, we use open-source [gymnasium](https://gymnasium.farama.org/) and [stable_baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html) packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26278663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install stable_baselines3==2.0.0a5\n",
    "!pip install gymnasium[box2d]\n",
    "!pip install swig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515de1b",
   "metadata": {},
   "source": [
    "Next, we import `gymnasium` and required `stable_baselines3` classes and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a03597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO  \n",
    "from stable_baselines3.common.evaluation import evaluate_policy  \n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257c836",
   "metadata": {},
   "source": [
    "In this example, we train multiple models with different learning rates. We can modify the next cell to train as many models while tunning different hyper paramters of the [`PPO` model](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a152a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different learning rates to use for training  \n",
    "learning_rates = [0.0003, 0.00025, 0.00035, 0.0004, 0.001, 0.0002, 0.0001]\n",
    "num_models = len(learning_rates) \n",
    "  \n",
    "# Create a list to store the PPO models  \n",
    "models = []  \n",
    "  \n",
    "# Create a vectorized Lunar Lander environment (stacked 16 environments)\n",
    "env = make_vec_env('LunarLander-v2', n_envs = 16)  \n",
    "\n",
    "# Train and save the PPO models\n",
    "# This is not the most efficient way of training, we can parallelize training using pyspark or muliprocessing\n",
    "for i in range(num_models):  \n",
    "    start_time = time.time()\n",
    "    print(f\"Learning rate: {learning_rates[i]}\")\n",
    "    model = PPO('MlpPolicy',\n",
    "                env,\n",
    "                verbose = 0,\n",
    "                n_steps = 2024,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.999,\n",
    "                gae_lambda = 0.99,\n",
    "                max_grad_norm = 9,\n",
    "                batch_size = 64,\n",
    "                learning_rate = learning_rates[i])  \n",
    "    model.learn(total_timesteps=1000000)  \n",
    "    model.save(f'ppo_model{i + 1}.zip')  # Save each model with a unique name  \n",
    "    models.append(model)  \n",
    "    print(f\"Finished training a model in {round((time.time() - start_time)/60.0, 3)} minutes...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e2bbe",
   "metadata": {},
   "source": [
    "Next, we evaluate the performance of each trained model by reviewing mean and std rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluation environment\n",
    "eval_env = Monitor(gym.make('LunarLander-v2'))\n",
    "\n",
    "# Calculate mean and std reward for each model\n",
    "for idx, m in enumerate(models):\n",
    "    mean_reward, std_reward = evaluate_policy(m, eval_env, n_eval_episodes=10, deterministic = True)\n",
    "    print(f\"Learning rate: {learning_rates[idx]} | Mean reward: {mean_reward} +/- {std_reward}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea14f1",
   "metadata": {},
   "source": [
    "Lastly, we **ensemble** the trained models in a single model and evaluate the performance of the **ensemble model**.\n",
    "\n",
    "Optionally, we can only consider models in our ensemble that has a minimum mean reward.\n",
    "\n",
    "We use majority voting for selecting the action by the ensemble of models as the action space in this example is discrete, however the `predict` function can be modified accordingly for continuous action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90267de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in the ensemble:  6\n",
      "Ensemble model: Mean reward = 252.70587512668658 +/- 28.829527834468042\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "\n",
    "MINIMUM_REWARD = 240\n",
    "\n",
    "# Create a custom ensemble model class  \n",
    "class EnsembleModel:  \n",
    "    def __init__(self, models):  \n",
    "        self.models = models  \n",
    "  \n",
    "    # To use this Ensemble model as a policy in the PPO class\n",
    "    # we need to define the predict function and pass relevant arguments\n",
    "    def predict(self, obs, **kwargs):  \n",
    "        actions = []\n",
    "        for model in self.models: \n",
    "            actions.append(model.predict(obs, **kwargs)[0].item())\n",
    "        return st.mode(actions, keepdims=True)  \n",
    "  \n",
    "\n",
    "# We would like to only consider models with a mean reward greater than a given threshold in our ensemble\n",
    "update_models = [m for m in models if \n",
    "                 evaluate_policy(m, eval_env, n_eval_episodes=10, deterministic = True)[0] >= MINIMUM_REWARD]\n",
    "print(\"Number of models in the ensemble: \", len(update_models))\n",
    "ensemble_model = EnsembleModel(update_models)  \n",
    "  \n",
    "# Calculate mean and std reward for the ensemble model\n",
    "mean_reward, std_reward = evaluate_policy(ensemble_model, eval_env, n_eval_episodes=10, deterministic = True)  # Replace n_eval_episodes with your desired number of evaluation episodes  \n",
    "  \n",
    "print(f\"Ensemble model: Mean reward = {mean_reward} +/- {std_reward}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c89aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
